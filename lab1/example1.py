# Lab: ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Excel ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô PyTorch Tensors
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Deep Learning

import pandas as pd
import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

print("PyTorch version:", torch.__version__)
print("Pandas version:", pd.__version__)
print("NumPy version:", np.__version__)

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô Excel Format (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö)
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")
print("="*60)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á - House Price Dataset
np.random.seed(42)
n_samples = 1000

data = {
    'area_sqft': np.random.normal(2000, 500, n_samples),
    'bedrooms': np.random.randint(1, 6, n_samples),
    'bathrooms': np.random.randint(1, 4, n_samples),
    'age_years': np.random.randint(0, 50, n_samples),
    'location': np.random.choice(['Downtown', 'Suburban', 'Rural'], n_samples),
    'garage': np.random.choice([0, 1], n_samples),
    'price': np.random.normal(300000, 100000, n_samples)
}

# ‡∏™‡∏£‡πâ‡∏≤‡∏á relationship ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÅ‡∏•‡∏∞ target
data['price'] = (data['area_sqft'] * 150 + 
                data['bedrooms'] * 20000 + 
                data['bathrooms'] * 15000 - 
                data['age_years'] * 1000 + 
                data['garage'] * 25000 + 
                np.random.normal(0, 20000, n_samples))

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
df = pd.DataFrame(data)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô Excel file
df.to_excel('house_prices.xlsx', index=False, sheet_name='Data')
print(f"‚úì ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå house_prices.xlsx ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
print(f"‚úì ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {len(df)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
print(f"‚úì ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features: {len(df.columns)-1}")

# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
print(f"\n‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 5 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å:")
print(df.head())

print(f"\n‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô:")
print(df.describe())

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Excel
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Excel")
print("="*60)

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
df_loaded = pd.read_excel('house_prices.xlsx', sheet_name='Data')
print(f"‚úì ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå Excel ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
print(f"Shape: {df_loaded.shape}")

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
selected_columns = ['area_sqft', 'bedrooms', 'bathrooms', 'price']
df_selected = pd.read_excel('house_prices.xlsx', 
                          sheet_name='Data', 
                          usecols=selected_columns)
print(f"‚úì ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å: {selected_columns}")

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡πÅ‡∏ñ‡∏ß
df_subset = pd.read_excel('house_prices.xlsx', 
                         sheet_name='Data', 
                         nrows=100)  # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏Ñ‡πà 100 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
print(f"‚úì ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 100 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å: Shape {df_subset.shape}")

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Data Preprocessing
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 3: Data Preprocessing")
print("="*60)

# ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ï‡πá‡∏°
df_work = df_loaded.copy()

# 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö missing values
print("Missing values:")
print(df_work.isnull().sum())

# 2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö data types
print(f"\nData types:")
print(df_work.dtypes)

# 3. Handle categorical data - Location
print(f"\nLocation categories: {df_work['location'].unique()}")

# One-hot encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö location
location_encoded = pd.get_dummies(df_work['location'], prefix='location')
df_work = pd.concat([df_work, location_encoded], axis=1)
df_work = df_work.drop('location', axis=1)

print(f"‚úì One-hot encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö location ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
print(f"New columns: {location_encoded.columns.tolist()}")

# 4. ‡πÅ‡∏¢‡∏Å features ‡πÅ‡∏•‡∏∞ target
feature_columns = [col for col in df_work.columns if col != 'price']
X = df_work[feature_columns]
y = df_work['price']

print(f"\nFeatures ({len(feature_columns)}): {feature_columns}")
print(f"Target: price")
print(f"X shape: {X.shape}, y shape: {y.shape}")

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô NumPy Arrays
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 4: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô NumPy Arrays")
print("="*60)

# ‡πÅ‡∏õ‡∏•‡∏á pandas DataFrame ‡πÄ‡∏õ‡πá‡∏ô NumPy
X_np = X.values
y_np = y.values

print(f"X_np shape: {X_np.shape}, dtype: {X_np.dtype}")
print(f"y_np shape: {y_np.shape}, dtype: {y_np.dtype}")

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
print(f"\n‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á X_np (3 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å):")
print(X_np[:3])
print(f"\n‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á y_np (5 ‡∏Ñ‡πà‡∏≤‡πÅ‡∏£‡∏Å): {y_np[:5]}")

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Feature Scaling/Normalization
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 5: Feature Scaling/Normalization")
print("="*60)

# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/test ‡∏Å‡πà‡∏≠‡∏ô scaling
X_train, X_test, y_train, y_test = train_test_split(
    X_np, y_np, test_size=0.2, random_state=42
)

print(f"Train set: X={X_train.shape}, y={y_train.shape}")
print(f"Test set: X={X_test.shape}, y={y_test.shape}")

# StandardScaler (Z-score normalization)
scaler_X = StandardScaler()
scaler_y = StandardScaler()

# Fit ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ training data
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()

print(f"‚úì Feature scaling ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
print(f"X_train_scaled - mean: {X_train_scaled.mean():.4f}, std: {X_train_scaled.std():.4f}")
print(f"y_train_scaled - mean: {y_train_scaled.mean():.4f}, std: {y_train_scaled.std():.4f}")

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô PyTorch Tensors
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 6: ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô PyTorch Tensors")
print("="*60)

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏à‡∏≤‡∏Å NumPy arrays
X_train_tensor = torch.from_numpy(X_train_scaled).float()
X_test_tensor = torch.from_numpy(X_test_scaled).float()
y_train_tensor = torch.from_numpy(y_train_scaled).float()
y_test_tensor = torch.from_numpy(y_test_scaled).float()

print(f"PyTorch Tensors:")
print(f"X_train_tensor: {X_train_tensor.shape}, dtype: {X_train_tensor.dtype}")
print(f"X_test_tensor: {X_test_tensor.shape}, dtype: {X_test_tensor.dtype}")
print(f"y_train_tensor: {y_train_tensor.shape}, dtype: {y_train_tensor.dtype}")
print(f"y_test_tensor: {y_test_tensor.shape}, dtype: {y_test_tensor.dtype}")

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á tensor ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
X_direct = torch.tensor(X_train_scaled, dtype=torch.float32)
y_direct = torch.tensor(y_train_scaled, dtype=torch.float32)

print(f"\nDirect tensor creation:")
print(f"X_direct: {X_direct.shape}, dtype: {X_direct.dtype}")

# ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà 3: ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å pandas ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
X_from_pandas = torch.from_numpy(X.values).float()
print(f"From pandas: {X_from_pandas.shape}")

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞ Visualization
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 7: ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞ Visualization")
print("="*60)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á tensor
print("Tensor properties:")
print(f"X_train_tensor requires_grad: {X_train_tensor.requires_grad}")
print(f"X_train_tensor device: {X_train_tensor.device}")
print(f"Memory usage: {X_train_tensor.element_size() * X_train_tensor.nelement() / 1024**2:.2f} MB")

# ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
print(f"\n‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
print(f"First sample: {X_train_tensor[0]}")
print(f"First 3 samples of feature 0: {X_train_tensor[:3, 0]}")
print(f"Feature statistics - min: {X_train_tensor.min():.4f}, max: {X_train_tensor.max():.4f}")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü
plt.figure(figsize=(15, 10))

# ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà 1: Distribution ‡∏Ç‡∏≠‡∏á target variable
plt.subplot(2, 3, 1)
plt.hist(y_train.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')
plt.title('House Price Distribution (Original)')
plt.xlabel('Price ($)')
plt.ylabel('Frequency')

# ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà 2: Distribution ‡∏´‡∏•‡∏±‡∏á scaling
plt.subplot(2, 3, 2)
plt.hist(y_train_scaled.flatten(), bins=50, alpha=0.7, color='red', edgecolor='black')
plt.title('House Price Distribution (Scaled)')
plt.xlabel('Scaled Price')
plt.ylabel('Frequency')

# ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà 3: Correlation matrix
plt.subplot(2, 3, 3)
correlation_matrix = np.corrcoef(X_train_scaled.T)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
           xticklabels=feature_columns, yticklabels=feature_columns)
plt.title('Feature Correlation Matrix')
plt.xticks(rotation=45)

# ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà 4: Feature distributions
plt.subplot(2, 3, 4)
for i in range(min(3, X_train_scaled.shape[1])):
    plt.hist(X_train_scaled[:, i], bins=30, alpha=0.5, 
             label=f'{feature_columns[i]}')
plt.legend()
plt.title('Feature Distributions (Scaled)')
plt.xlabel('Scaled Values')
plt.ylabel('Frequency')

# ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà 5: Scatter plot
plt.subplot(2, 3, 5)
plt.scatter(X_train[:, 0], y_train, alpha=0.5, s=10)
plt.xlabel('Area (sqft)')
plt.ylabel('Price ($)')
plt.title('Area vs Price')

# ‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà 6: Data summary
plt.subplot(2, 3, 6)
summary_data = {
    'Total Samples': len(df),
    'Training Samples': len(X_train),
    'Test Samples': len(X_test),
    'Features': X_train.shape[1],
    'Memory (MB)': X_train_tensor.element_size() * X_train_tensor.nelement() / 1024**2
}

bars = plt.bar(range(len(summary_data)), list(summary_data.values()))
plt.xticks(range(len(summary_data)), list(summary_data.keys()), rotation=45)
plt.title('Dataset Summary')

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡πà‡∏≤‡∏ö‡∏ô bars
for bar, value in zip(bars, summary_data.values()):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.01,
             f'{value:.1f}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig('data_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 8: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö PyTorch
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 8: ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader")
print("="*60)

from torch.utils.data import TensorDataset, DataLoader

# ‡∏™‡∏£‡πâ‡∏≤‡∏á TensorDataset
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print(f"‚úì DataLoader ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")
print(f"Training batches: {len(train_loader)}")
print(f"Test batches: {len(test_loader)}")

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö DataLoader
for batch_idx, (data, target) in enumerate(train_loader):
    print(f"Batch {batch_idx + 1}: X shape = {data.shape}, y shape = {target.shape}")
    if batch_idx == 2:  # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 3 batch ‡πÅ‡∏£‡∏Å
        break

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 9: ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö ANN
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 9: ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏±‡∏ö ANN")
print("="*60)

import torch.nn as nn

# ‡∏™‡∏£‡πâ‡∏≤‡∏á simple neural network
class HousePricePredictor(nn.Module):
    def __init__(self, input_features):
        super(HousePricePredictor, self).__init__()
        self.linear = nn.Linear(input_features, 1)
    
    def forward(self, x):
        return self.linear(x)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á model
model = HousePricePredictor(input_features=X_train_tensor.shape[1])
print(f"Model created: {X_train_tensor.shape[1]} inputs ‚Üí 1 output")

# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö forward pass
with torch.no_grad():
    sample_prediction = model(X_train_tensor[:5])
    print(f"Sample predictions: {sample_prediction.flatten()}")
    print(f"Actual values: {y_train_tensor[:5]}")

# =============================================================================
# ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 10: ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞ Best Practices
# =============================================================================

print("\n" + "="*60)
print("‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà 10: ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞ Best Practices")
print("="*60)

summary = f"""
üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:

1. ‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Excel: {df.shape[0]:,} samples, {df.shape[1]} columns
2. ‚úÖ Data preprocessing: One-hot encoding, scaling
3. ‚úÖ Train/Test split: {len(X_train)}/{len(X_test)} samples
4. ‚úÖ Feature scaling: StandardScaler
5. ‚úÖ ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô PyTorch tensors: {X_train_tensor.dtype}
6. ‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á DataLoader: batch_size={batch_size}
7. ‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö Neural Network

üéØ Best Practices:
- ‡πÉ‡∏ä‡πâ .float() ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tensors ‡πÉ‡∏ô neural networks
- Scale features ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô tensors
- ‡πÉ‡∏ä‡πâ DataLoader ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö batch processing
- ‡πÄ‡∏Å‡πá‡∏ö scalers ‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö inverse transform
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö tensor shapes ‡πÅ‡∏•‡∏∞ dtypes ‡πÄ‡∏™‡∏°‡∏≠

üíæ Memory Usage: {X_train_tensor.element_size() * X_train_tensor.nelement() / 1024**2:.2f} MB
"""

print(summary)

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scalers ‡πÅ‡∏•‡∏∞ tensors
torch.save({
    'X_train_tensor': X_train_tensor,
    'X_test_tensor': X_test_tensor,
    'y_train_tensor': y_train_tensor,
    'y_test_tensor': y_test_tensor,
    'scaler_X': scaler_X,
    'scaler_y': scaler_y,
    'feature_columns': feature_columns
}, 'processed_data.pth')

print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ô 'processed_data.pth'")

print("\nüéâ Lab ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ train neural networks")